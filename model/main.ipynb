{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d207d80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s shape:  torch.Size([1, 16, 180, 320])\n",
      "f1 shape:  torch.Size([1, 64, 90, 160])\n",
      "f2 shape:  torch.Size([1, 128, 45, 80])\n",
      "f3 shape:  torch.Size([1, 256, 22, 40])\n",
      "f4 shape:  torch.Size([1, 512, 11, 20])\n",
      "h1 shape unpooled:  torch.Size([1, 512, 22, 40])\n",
      "f3 shape:  torch.Size([1, 256, 22, 40])\n",
      "torch.Size([1, 768, 22, 40])\n",
      "h2 :  torch.Size([1, 128, 22, 40])\n",
      "h2 shape unpooled:  torch.Size([1, 128, 45, 80])\n",
      "f2 shape:  torch.Size([1, 128, 45, 80])\n",
      "torch.Size([1, 256, 45, 80])\n",
      "h3 shape unpooled:  torch.Size([1, 32, 90, 160])\n",
      "f1 shape:  torch.Size([1, 64, 90, 160])\n",
      "score shape:  torch.Size([1, 1, 360, 640])\n",
      "tensor([[[[0.6056, 0.6044, 0.6032,  ..., 0.6073, 0.6081, 0.6088],\n",
      "          [0.6066, 0.6055, 0.6044,  ..., 0.6078, 0.6082, 0.6087],\n",
      "          [0.6076, 0.6066, 0.6056,  ..., 0.6082, 0.6084, 0.6086],\n",
      "          ...,\n",
      "          [0.6101, 0.6100, 0.6098,  ..., 0.6077, 0.6082, 0.6087],\n",
      "          [0.6101, 0.6098, 0.6096,  ..., 0.6067, 0.6072, 0.6077],\n",
      "          [0.6101, 0.6097, 0.6093,  ..., 0.6057, 0.6062, 0.6067]]]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "geo map : torch.Size([1, 8, 360, 640])\n",
      "tensor([[[[ 0.4767,  0.4774,  0.4780,  ...,  0.4758,  0.4754,  0.4750],\n",
      "          [ 0.4762,  0.4768,  0.4773,  ...,  0.4756,  0.4753,  0.4751],\n",
      "          [ 0.4756,  0.4762,  0.4767,  ...,  0.4753,  0.4752,  0.4751],\n",
      "          ...,\n",
      "          [ 0.4743,  0.4744,  0.4745,  ...,  0.4756,  0.4753,  0.4751],\n",
      "          [ 0.4743,  0.4745,  0.4746,  ...,  0.4761,  0.4759,  0.4756],\n",
      "          [ 0.4743,  0.4745,  0.4748,  ...,  0.4767,  0.4764,  0.4761]],\n",
      "\n",
      "         [[-0.7527, -0.7531, -0.7535,  ..., -0.7521, -0.7519, -0.7516],\n",
      "          [-0.7524, -0.7527, -0.7531,  ..., -0.7520, -0.7518, -0.7516],\n",
      "          [-0.7520, -0.7523, -0.7527,  ..., -0.7518, -0.7517, -0.7517],\n",
      "          ...,\n",
      "          [-0.7511, -0.7512, -0.7513,  ..., -0.7520, -0.7518, -0.7516],\n",
      "          [-0.7512, -0.7512, -0.7513,  ..., -0.7523, -0.7522, -0.7520],\n",
      "          [-0.7512, -0.7513, -0.7514,  ..., -0.7527, -0.7525, -0.7523]],\n",
      "\n",
      "         [[ 1.3034,  1.3023,  1.3011,  ...,  1.3051,  1.3059,  1.3066],\n",
      "          [ 1.3044,  1.3034,  1.3023,  ...,  1.3056,  1.3060,  1.3065],\n",
      "          [ 1.3054,  1.3045,  1.3035,  ...,  1.3060,  1.3062,  1.3064],\n",
      "          ...,\n",
      "          [ 1.3079,  1.3078,  1.3076,  ...,  1.3055,  1.3060,  1.3065],\n",
      "          [ 1.3079,  1.3076,  1.3073,  ...,  1.3045,  1.3050,  1.3055],\n",
      "          [ 1.3079,  1.3075,  1.3071,  ...,  1.3035,  1.3040,  1.3045]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.6466,  0.6456,  0.6447,  ...,  0.6480,  0.6486,  0.6492],\n",
      "          [ 0.6474,  0.6465,  0.6457,  ...,  0.6484,  0.6487,  0.6491],\n",
      "          [ 0.6483,  0.6475,  0.6466,  ...,  0.6488,  0.6489,  0.6490],\n",
      "          ...,\n",
      "          [ 0.6503,  0.6502,  0.6501,  ...,  0.6483,  0.6487,  0.6491],\n",
      "          [ 0.6503,  0.6501,  0.6498,  ...,  0.6475,  0.6479,  0.6483],\n",
      "          [ 0.6503,  0.6499,  0.6496,  ...,  0.6467,  0.6471,  0.6475]],\n",
      "\n",
      "         [[ 0.4382,  0.4380,  0.4378,  ...,  0.4386,  0.4387,  0.4388],\n",
      "          [ 0.4384,  0.4382,  0.4380,  ...,  0.4386,  0.4387,  0.4388],\n",
      "          [ 0.4386,  0.4384,  0.4382,  ...,  0.4387,  0.4388,  0.4388],\n",
      "          ...,\n",
      "          [ 0.4391,  0.4390,  0.4390,  ...,  0.4386,  0.4387,  0.4388],\n",
      "          [ 0.4391,  0.4390,  0.4390,  ...,  0.4384,  0.4385,  0.4386],\n",
      "          [ 0.4391,  0.4390,  0.4389,  ...,  0.4383,  0.4383,  0.4384]],\n",
      "\n",
      "         [[-0.9956, -0.9946, -0.9936,  ..., -0.9971, -0.9977, -0.9983],\n",
      "          [-0.9964, -0.9955, -0.9946,  ..., -0.9974, -0.9978, -0.9982],\n",
      "          [-0.9973, -0.9965, -0.9956,  ..., -0.9978, -0.9980, -0.9981],\n",
      "          ...,\n",
      "          [-0.9994, -0.9993, -0.9992,  ..., -0.9974, -0.9978, -0.9982],\n",
      "          [-0.9994, -0.9992, -0.9989,  ..., -0.9965, -0.9969, -0.9974],\n",
      "          [-0.9994, -0.9990, -0.9987,  ..., -0.9957, -0.9961, -0.9965]]]],\n",
      "       grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import cv2\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset , DataLoader, random_split\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from imgDataset import imgDataset , custom_collate\n",
    "from EAST import EAST\n",
    "from train import train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9239ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b0a830d",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "directory_train = os.getenv('Directory_train')\n",
    "directory_train_textAndCoords = os.getenv('directory_train_textAndCoords')\n",
    "directory_test = os.getenv('Directory_test')\n",
    "directory_test_textAndCoords = os.getenv('directory_test_textAndCoords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c18ca85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path_imgs and path_labels\n",
    "path_imgs = os.listdir(directory_train)\n",
    "path_labels = os.listdir(directory_train_textAndCoords)\n",
    "\n",
    "# Generate paths for training images and labels\n",
    "train_img_paths = [os.path.join(directory_train, f) for f in path_imgs]\n",
    "train_label_paths = [os.path.join(directory_train_textAndCoords, f) for f in path_labels]\n",
    "\n",
    "#print(train_img_paths)\n",
    "#print(train_label_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9be443f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([1, 360, 640])\n",
      "First Coords: [[243, 108, 277, 105, 279, 121, 244, 124], [0, 180, 43, 177, 45, 192, 0, 200], [24, 197, 31, 196, 32, 203, 25, 205], [132, 117, 143, 113, 147, 121, 133, 127]]\n",
      "First Coords:4\n"
     ]
    }
   ],
   "source": [
    "train_dataset = [train_img_paths, train_label_paths]\n",
    "train_dataset = imgDataset(train_img_paths, train_label_paths)\n",
    "img, labels, coords = train_dataset[3]\n",
    "\n",
    "print(f\"Image shape: {img.shape}\") # Should be [1, 360, 360] since it's grayscale\n",
    "print(f\"First Coords: {coords}\")\n",
    "print(f\"First Coords:{len(coords)}\")\n",
    "\n",
    "'''max_len = max(len(train_dataset[j][2]) for j in range(len(train_dataset)))\n",
    "pad_len = max_len - len(train_dataset[3][2])\n",
    "print(pad_len)'''\n",
    "\n",
    "batch = custom_collate(train_dataset)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "effd0224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[188, 58, 231, 58, 232, 65, 189, 65], [246, 57, 259, 57, 259, 65, 246, 65], [187, 77, 204, 77, 204, 85, 187, 85], [246, 75, 275, 75, 275, 85, 246, 85], [188, 99, 211, 99, 211, 106, 188, 106], [247, 95, 269, 94, 269, 102, 247, 103], [187, 0, 247, 0, 246, 42, 186, 43], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "print(batch[2][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33bddb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spltining the dataset into training and validation sets\n",
    "val_split = 0.2\n",
    "training_size = int((1 - val_split) * len(train_dataset))\n",
    "val_size = len(train_dataset) - training_size\n",
    "training_dataset, val_dataset = random_split(train_dataset, [training_size, val_size])\n",
    "loader_train = DataLoader(training_dataset, batch_size=32, shuffle=True)\n",
    "loader_val = DataLoader(val_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "#print(f\"loader_train: {loader_train}\")\n",
    "#print(f\"loader_val: {loader_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "679a640c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EAST(\n",
       "  (Feature_extractor_start): Sequential(\n",
       "    (0): Conv2d(1, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (Feature_extractor_1): Sequential(\n",
       "    (0): Conv2d(16, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (Feature_extractor_2): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (Feature_extractor_3): Sequential(\n",
       "    (0): Conv2d(128, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (Feature_extractor_4): Sequential(\n",
       "    (0): Conv2d(256, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (Feature_merging_4): Sequential(\n",
       "    (0): Conv2d(768, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU(inplace=True)\n",
       "  )\n",
       "  (Feature_merging_3): Sequential(\n",
       "    (0): Conv2d(384, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU(inplace=True)\n",
       "  )\n",
       "  (Feature_merging_2): Sequential(\n",
       "    (0): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU(inplace=True)\n",
       "  )\n",
       "  (Feature_extractor_end): Sequential(\n",
       "    (0): Conv2d(96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (output_start): Sequential(\n",
       "    (0): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (output_score_map): Sequential(\n",
       "    (0): Conv2d(1, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): Conv2d(4, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (output_score_quad_geometry): Sequential(\n",
       "    (0): Conv2d(1, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = EAST(color_channel=1)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98d8f1dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model not load, starting from scratch\n"
     ]
    }
   ],
   "source": [
    "#loasding model if it exists\n",
    "if os.getenv('Load_model') == 'True': \n",
    "    model.load_state_dict(torch.load(os.getenv('model_path')))\n",
    "    print(\"Model loaded successfully\")\n",
    "else:\n",
    "    print(\"Model not load, starting from scratch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65a6f194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cycle: 1/12\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "each element in list of batch should be of equal size",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m optimizer = optim.Adam(model.parameters(), lr = \u001b[32m0.001\u001b[39m)\n\u001b[32m      4\u001b[39m cycles = \u001b[32m12\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloader_train\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mloader_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloader_val\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mloader_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcycles\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcycles\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Balan\\OneDrive\\Documents\\projects\\EAST_model_Pytorch\\model\\train.py:19\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, loader_train, loader_val, criterion, optimizer, cycles)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m cycle \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(cycles):\n\u001b[32m     18\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mcycle: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcycle+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcycles\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     train_loss, train_acc = \u001b[43mloop_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m                                        \u001b[49m\u001b[43mdataset_loaded\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mloader_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m                                        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m                                        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m                                        \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m                                        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m     val_loss, val_acc = loop_helper(model,\n\u001b[32m     26\u001b[39m                                         dataset_loaded = loader_train, \n\u001b[32m     27\u001b[39m                                         device = device, \n\u001b[32m     28\u001b[39m                                         optimizer = optimizer, \n\u001b[32m     29\u001b[39m                                         criterion = criterion, \n\u001b[32m     30\u001b[39m                                         train = \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     31\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTrain Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Train Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m%\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Balan\\OneDrive\\Documents\\projects\\EAST_model_Pytorch\\model\\train.py:54\u001b[39m, in \u001b[36mloop_helper\u001b[39m\u001b[34m(model, dataset_loaded, device, optimizer, criterion, train)\u001b[39m\n\u001b[32m     51\u001b[39m running_loss = \u001b[32m0.0\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m#cycles through the photos and coords in each batch\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoords\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataset_loaded\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcoords\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoords\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Balan\\OneDrive\\Documents\\projects\\EAST_model_Pytorch\\model\\venv-east\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Balan\\OneDrive\\Documents\\projects\\EAST_model_Pytorch\\model\\venv-east\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:788\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    787\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    789\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    790\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Balan\\OneDrive\\Documents\\projects\\EAST_model_Pytorch\\model\\venv-east\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Balan\\OneDrive\\Documents\\projects\\EAST_model_Pytorch\\model\\venv-east\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:398\u001b[39m, in \u001b[36mdefault_collate\u001b[39m\u001b[34m(batch)\u001b[39m\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdefault_collate\u001b[39m(batch):\n\u001b[32m    338\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    339\u001b[39m \u001b[33;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[32m    340\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    396\u001b[39m \u001b[33;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[32m    397\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Balan\\OneDrive\\Documents\\projects\\EAST_model_Pytorch\\model\\venv-east\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:223\u001b[39m, in \u001b[36mcollate\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    221\u001b[39m     clone = copy.copy(elem)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, samples \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(transposed):\n\u001b[32m--> \u001b[39m\u001b[32m223\u001b[39m         clone[i] = \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    224\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m clone\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Balan\\OneDrive\\Documents\\projects\\EAST_model_Pytorch\\model\\venv-east\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:207\u001b[39m, in \u001b[36mcollate\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    205\u001b[39m elem_size = \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mnext\u001b[39m(it))\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28mlen\u001b[39m(elem) == elem_size \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m it):\n\u001b[32m--> \u001b[39m\u001b[32m207\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33meach element in list of batch should be of equal size\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    208\u001b[39m transposed = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(*batch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n",
      "\u001b[31mRuntimeError\u001b[39m: each element in list of batch should be of equal size"
     ]
    }
   ],
   "source": [
    "#defualt model otipions\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "cycles = 12\n",
    "train_model(model = model, loader_train = loader_train, loader_val = loader_val, criterion = criterion, optimizer = optimizer, cycles = cycles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591bc0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = os.getenv('Model_save_path', 'east_model.pth')\n",
    "torch.save(model.state_dict(), model_save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-east",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
